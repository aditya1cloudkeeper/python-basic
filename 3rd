
        # import requests
        # import time
        # import logging

        # def setup_logging():

        #     logging.basicConfig(
        #         filename='uptime_monitor.log',  # Log file name
        #         level=logging.INFO,             # Log INFO and above (INFO, ERROR, etc.)
        #         format='%(asctime)s - %(levelname)s - %(message)s'
        #     )

        # def check_url(url):

        #     try:
        #         response = requests.get(url, timeout=5)  # 5-second timeout for the request
        #         return response.status_code, response.reason
        #     except Exception as error:
        #         return None, str(error)

        # def monitor_urls(urls, base_interval=10):

        #     # Track consecutive error counts for each URL.
        #     error_counts = {url: 0 for url in urls}
        
        #     # Infinite loop: keep checking the URLs at regular intervals.
        #     while True:
        #         # Determine the maximum error count among all URLs (used for backoff timing).
        #         max_error_count = 0
                
        #         # Check each URL one by one.
        #         for url in urls:
        #             print(f"Checking URL: {url}")
        #             logging.info(f"Checking URL: {url}")
                
        #             status_code, message = check_url(url)
                
        #             # If we didn't get a valid status code (e.g., due to a timeout or connection error)
        #             if status_code is None:
        #                 error_msg = f"Error checking {url}: {message}"
        #                 print(error_msg)
        #                 logging.error(error_msg)
        #                 error_counts[url] += 1  # Increase error count
        #             else:
        #                 # Format the status code and message nicely.
        #                 full_status = f"{status_code} {message}"
        #                 print(f"Status Code: {full_status}")
        #                 logging.info(f"URL: {url} returned {full_status}")
                        
        #                 # Check if the status indicates an error (client error 4xx or server error 5xx)
        #                 if 400 <= status_code < 600:
        #                     if 400 <= status_code < 500:
        #                         alert = f"ALERT: 4xx error encountered for URL: {url}"
        #                     else:
        #                         alert = f"ALERT: 5xx error encountered for URL: {url}"
        #                     print(alert)
        #                     logging.error(alert)
        #                     error_counts[url] += 1
        #                 else:
        #                     print("The website is UP and running.\n")
        #                     error_counts[url] = 0  # Reset error count if the check was successful

        #             # Update the maximum error count seen (for calculating backoff)
        #             max_error_count = max(max_error_count, error_counts[url])
        #             print()  # Blank line for better readability

        #         # Use exponential backoff: wait longer if there have been several consecutive errors.
        #         wait_time = base_interval * (2 ** max_error_count)
        #         print(f"Waiting for {wait_time} seconds before the next check...\n")
        #         time.sleep(wait_time)

        # if __name__ == "__main__":
        #     setup_logging()  # Initialize the log file
    
        #       # List of URLs to be monitored.
        #     url_list = [
        #         "http://www.example.com/nonexistentpage",  # Expected: 4xx error
        #         "http://httpstat.us/404",                   # Expected: 4xx error
        #         "http://httpstat.us/500",                   # Expected: 5xx error
        #         "https://www.google.com/"                   # Expected: 200 OK
        #     ]
    
        #     # Start the monitoring loop 
        #     monitor_urls(url_list, base_interval=10)
